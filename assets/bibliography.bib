% Encoding: UTF-8

@Misc{CC0License,
  author = {{Creative Commons}},
  title  = {{CC0 1.0 Universal}},
  month  = jan,
  year   = {2018},
  note   = {Accessed: 2018-01-13},
  url    = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
}

@Article{Dalmaijer2014,
  author   = {Edwin Dalmaijer and Sebastiaan Math\^ot and Stefan van der Stigchel},
  title    = {{PyGaze}: an open-source, cross-platform toolbox for minimal-effort programming of eye tracking experiments},
  journal  = {Behavior Research Methods},
  year     = {2014},
  volume   = {46},
  number   = {4},
  pages    = {913--921},
  month    = dec,
  abstract = {The PyGaze toolbox is an open-source software package for Python, a high-level programming language. It is designed for creating eyetracking experiments in Python syntax with the least possible effort, and it offers programming ease and script readability without constraining functionality and flexibility. PyGaze can be used for visual and auditory stimulus presentation; for response collection via keyboard, mouse, joystick, and other external hardware; and for the online detection of eye movements using a custom algorithm. A wide range of eyetrackers of different brands (EyeLink, SMI, and Tobii systems) are supported. The novelty of PyGaze lies in providing an easy-to-use layer on top of the many different software libraries that are required for implementing eyetracking experiments. Essentially, PyGaze is a software bridge for eyetracking research.},
  doi      = {10.3758/s13428-013-0422-2},
  keywords = {Eyetracking, Open-source, Software, Python, PsychoPy, Gaze contingency, read},
  url      = {https://link.springer.com/article/10.3758%2Fs13428-013-0422-2},
}

@InCollection{Davson2017,
  author    = {Hugh Davson},
  title     = {Human Eye},
  booktitle = {Encyclopædia Britannica},
  publisher = {Encyclopædia Britannica, inc.},
  year      = {2017},
  month     = sep,
  note      = {Accessed: 2017-12-11},
  abstract  = {Human eye, in humans, specialized sense organ capable of receiving visual images, which are then carried to the brain.},
  url       = {https://www.britannica.com/science/human-eye},
}

@Misc{Driessen2010,
  author = {Vincent Driessen},
  title  = {A successful Git branching model},
  month  = jan,
  year   = {2010},
  note   = {Accessed: 2018-01-02},
  url    = {http://nvie.com/posts/a-successful-git-branching-model/},
}

@Misc{Facebase,
  author       = {Seth M. Weinberg and Mary L. Marazita},
  title        = {3D Facial Norms Database},
  howpublished = {https://www.facebase.org},
  month        = nov,
  year         = {2009},
  note         = {Accessed: 2018-01-05. NIDCR Grant: 1-U01-DE020078},
  abstract     = {Although ample evidence exists that facial appearance and structure are highly heritable, there is a dearth of information regarding how variation in specific genes relates to the diversity of facial forms evident in our species. With the advent of affordable, non-invasive 3D surface imaging technology, it is now possible to capture detailed quantitative information about the face in a large number of individuals. By coupling state- of-the-art 3D imaging with advances in high-throughput genotyping, an unparalleled opportunity exists to map the genetic determinants of normal facial variation. An improved understanding of the relationship between genotype and facial phenotype may help illuminate the factors influencing liability to common craniofacial anomalies, particularly orofacial clefts, which are among the most prevalent birth defects in humans.},
  url          = {https://www.facebase.org/facial_norms},
}

@InProceedings{Filho2010,
  author    = {H\'elio Perroni Filho and Alberto Ferreira De Souza},
  title     = {{VG-RAM WNN} approach to monocular depth perception},
  booktitle = {Neural Information Processing. Models and Applications},
  year      = {2010},
  editor    = {Kok Wai Wong and Balapuwaduge Sumudu Udaya Mendis and Abdesselam Bouzerdoum},
  volume    = {6444},
  series    = {Lecture Notes in Computer Science},
  pages     = {509--516},
  address   = {Berlin/Heidelberg, Germany},
  month     = nov,
  publisher = {Springer},
  abstract  = {We have examined Virtual Generalizing Random Access Memory Weightless Neural Networks (VG-RAM WNN) as platform for depth map inference from static monocular images. For that, we have designed, implemented and compared the performance of VG-RAM WNN systems against that of depth estimation systems based on Markov Random Field (MRF) models. While not surpassing the performance of such systems, our results are consistent to theirs, and allow us to infer important features of the human visual cortex.},
  doi       = {10.1007/978-3-642-17534-3_63},
  keywords  = {Monocular depth perception, weightless neural networks},
  url       = {https://www.researchgate.net/publication/220000031_VG-RAM_WNN_approach_to_monocular_depth_perception},
}

@Book{Helmholtz1866,
  title     = {{Handbuch der physiologischen Optik} [{H}andbook of physiological optics]},
  publisher = {Gustav Karsten},
  year      = {1866},
  author    = {Hermann Helmholtz},
  editor    = {P. W. Brix and G. Decher and F. C. O. von Feilitzsch and F. Grashof and F. Harms and H. Helmholtz and G. Karsten and H. Karsten and C. Kuhn and J. Lamont and J. Pfeffer and E. E. Schmid and F. Schulz and L. Seidel and G. Weyer and W. Wundt},
  volume    = {IX},
  series    = {Allgemeine Encyklopädie der Physik},
  address   = {Leipzig},
  note      = {Allgemeine Encyklopädie der Physik},
  url       = {https://archive.org/details/handbuchderphysi00helm},
}

@Book{Huey1908,
  title     = {The psychology and pedagogy of reading},
  publisher = {The Macmillian Company},
  year      = {1908},
  author    = {Edmund Burke Huey},
  address   = {New York},
  month     = jan,
  url       = {https://archive.org/details/psychologyandpe00hueygoog},
}

@Misc{Hume2012,
  author = {Tristan Hume},
  title  = {Simple, accurate eye center tracking in {OpenCV}},
  month  = nov,
  year   = {2012},
  note   = {Accessed: 2018-01-11},
  url    = {http://thume.ca/projects/2012/11/04/simple-accurate-eye-center-tracking-in-opencv/},
}

@InProceedings{Judd2009,
  author    = {Tilke Judd and Krista Ehinger and Fr\'edo Durand and Antonio Torralba},
  title     = {{Learning to Predict Where Humans Look}},
  booktitle = {IEEE 12th International Conference on Computer Vision (ICCV)},
  year      = {2009},
  pages     = {2106--2113},
  month     = sep,
  abstract  = {For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.},
  doi       = {10.1109/ICCV.2009.5459462},
  url       = {http://people.csail.mit.edu/tjudd/WherePeopleLook/},
}

@Article{Jun2016,
  author   = {Dongwook Jun and Jong Man Lee and Su Yeong Gwon and Weiyuan Pan and Hyeon Chang Lee and Kang Ryoung Park and Hyun-Cheol Kim},
  title    = {{Compensation Method of Natural Head Movement for Gaze Tracking System Using an Ultrasonic Sensor for Distance Measurement}},
  journal  = {Sensors},
  year     = {2016},
  volume   = {16},
  number   = {110},
  month    = jan,
  abstract = {Most gaze tracking systems are based on the pupil center corneal reflection (PCCR) method using near infrared (NIR) illuminators. One advantage of the PCCR method is the high accuracy it achieves in gaze tracking because it compensates for the pupil center position based on the relative position of corneal specular reflection (SR). However, the PCCR method only works for user head movements within a limited range, and its performance is degraded by the natural movement of the user's head. To overcome this problem, we propose a gaze tracking method using an ultrasonic sensor that is robust to the natural head movement of users. Experimental results demonstrate that with our compensation method the gaze tracking system is more robust to natural head movements compared to other systems without our method and commercial systems.},
  doi      = {10.3390/s16010110},
  keywords = {gaze tracking system, compensation of head movements, ultrasonic sensor, natural head movement},
  url      = {http://www.mdpi.com/1424-8220/16/1/110},
}

@Article{King2009,
  author   = {Davis E. King},
  title    = {{Dlib-ml: A Machine Learning Toolkit}},
  journal  = {Journal of Machine Learning Research},
  year     = {2009},
  volume   = {10},
  pages    = {1755--1758},
  month    = jul,
  note     = {\url{http://dlib.net/}},
  abstract = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classifi- cation, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
  keywords = {kernel-methods, svm, rvm, kernel clustering, C++, Bayesian networks},
  url      = {http://jmlr.csail.mit.edu/papers/volume10/king09a/king09a.pdf},
}

@InProceedings{Krafka2016,
  author    = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
  title     = {Eye Tracking for Everyone},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {2176--2184},
  publisher = {IEEE},
  abstract  = {From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2:5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.},
  doi       = {10.1109/CVPR.2016.239},
  url       = {http://gazecapture.csail.mit.edu/index.php},
}

@Misc{Mallick2016,
  author = {Satya Mallick},
  title  = {Head Pose Estimation using OpenCV and Dlib},
  month  = sep,
  year   = {2016},
  note   = {Accessed: 2018-01-10},
  url    = {https://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/},
}

@Misc{MITLicense,
  author = {{Open Source Initiative}},
  title  = {The {MIT} License},
  month  = jan,
  year   = {2018},
  note   = {Accessed: 2018-01-01},
  url    = {https://opensource.org/licenses/MIT},
}

@InProceedings{PaulaVeronese2012,
  author    = {Lucas de Paula Veronese and Lauro Jos\'e Lyrio Junior and Filipe Wall Mutz and Jorcy de Oliveira Neto and Vitor Barbirato Azevedo and Mariella Berger and Alberto Ferreira De Souza and Claudine Badue},
  title     = {{Stereo matching with VG-RAM Weightless Neural Networks}},
  booktitle = {12th International Conference on Intelligent Systems Design and Applications (ISDA)},
  year      = {2012},
  editor    = {Ajith Abraham and Albert Zomaya and Sebastian Ventura and Ronald Yager and Vaclav Snasel and Azah Kamilah Muda and Philip Samuel},
  pages     = {309--314},
  address   = {Kochi, India},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Virtual Generalizing Random Access Memory Weightless Neural Networks (VG-RAM WNN) is an effective machine learning technique that offers simple implementation and fast training and test. We examined the performance of VG-RAM WNN on binocular dense stereo matching using the Middlebury Stereo Datasets. Our experimental results showed that, even without tackling occlusions and discontinuities in the stereo image pairs examined, our VG-RAM WNN architecture for stereo matching was able to rank at 114th position in the Middlebury Stereo Evaluation system. This result is promising, because the difference in performance among approaches ranked in distinct positions is very small.},
  doi       = {10.1109/ISDA.2012.6416556},
  keywords  = {Binocular Dense Stereo Matching, VG-RAM Weightless Neural Networks, Middlebury Stereo Vision Page},
  url       = {https://www.researchgate.net/publication/261468052_Stereo_matching_with_VG-RAM_Weightless_Neural_Networks},
}

@InProceedings{Roussos2012,
  author       = {Anastasios Roussos and Chris Russell and Ravi Garg and Lourdes Agapito},
  title        = {{Dense Multibody Motion Estimation and Reconstruction from a Handheld Camera}},
  booktitle    = {2013 IEEE International Symposium on Mixed and Augmented Reality},
  year         = {2012},
  pages        = {31--40},
  address      = {Atlanta, GA, USA},
  month        = nov,
  organization = {IEEE},
  publisher    = {Piscataway},
  abstract     = {Existing approaches to camera tracking and reconstruction from a single handheld camera for Augmented Reality (AR) focus on the reconstruction of static scenes. However, most real world scenarios are dynamic and contain multiple independently moving rigid objects. This paper addresses the problem of simultaneous segmentation, motion estimation and dense 3D reconstruction of dynamic scenes. We propose a dense solution to all three elements of this problem: depth estimation, motion label assignment and rigid transformation estimation directly from the raw video by optimizing a single cost function using a hill-climbing approach. We do not require prior knowledge of the number of objects present in the scene -- the number of independent motion models and their parameters are automatically estimated. The resulting inference method combines the best techniques in discrete and continuous optimization: a state of the art variational approach is used to estimate the dense depth maps while the motion segmentation is achieved using discrete graph-cut based optimization. For the rigid motion estimation of the independently moving objects we propose a novel tracking approach designed to cope with the small fields of view they induce and agile motion. Our experimental results on real sequences show how accurate segmentations and dense depth maps can be obtained in a completely automated way and used in marker-free AR applications.},
  doi          = {10.1109/ISMAR.2012.6402535},
  keywords     = {Cameras, Optimization, Motion segmentation, Image reconstruction, Tracking, Estimation, Motion estimation},
  url          = {https://www.researchgate.net/publication/233981640_Dense_Multibody_Motion_Estimation_and_Reconstruction_from_a_Handheld_Camera},
}

@Article{Sagonas2016,
  author   = {Christos Sagonas and Epameinondas Antonakos and Georgios Tzimiropoulos and Stefanos Zafeiriou and Maja Pantic},
  title    = {300 Faces In-The-Wild Challenge: database and results},
  journal  = {Image and Vision Computing},
  year     = {2016},
  volume   = {47},
  pages    = {3--18},
  note     = {Special Issue on Facial Landmark Localisation ``In-The-Wild''},
  abstract = {Computer Vision has recently witnessed great research advance towards automatic facial points detection. Numerous methodologies have been proposed during the last few years that achieve accurate and efficient performance. However, fair comparison between these methodologies is infeasible mainly due to two issues. (a) Most existing databases, captured under both constrained and unconstrained (in-the-wild) conditions have been annotated using different mark-ups and, in most cases, the accuracy of the annotations is low. (b) Most published works report experimental results using different training/testing sets, different error met- rics and, of course, landmark points with semantically different locations. In this paper, we aim to overcome the aforementioned problems by (a) proposing a semi-automatic annotation technique that was employed to re-annotate most existing facial databases under a unified protocol, and (b) presenting the 300 Faces In- The-Wild Challenge (300-W), the first facial landmark localization challenge that was organized twice, in 2013 and 2015. To the best of our knowledge, this is the first effort towards a unified annotation scheme of massive databases and a fair experimental comparison of existing facial landmark localization systems. The images and annotations of the new testing database that was used in the 300-W challenge are available from http://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/.},
  doi      = {10.1016/j.imavis.2016.01.002},
  keywords = {Facial landmark localization, Challenge, Semi-automatic annotation tool, Facial database},
  url      = {https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/},
}

@InProceedings{Stevens2017,
  author    = {Marc Stevens and Elie Bursztein and Pierre Karpman and Ange Albertini and Yarik Markov},
  title     = {The first collision for full {SHA-1}},
  booktitle = {Advances in Cryptology – CRYPTO 2017},
  year      = {2017},
  editor    = {J. Katz and H. Shacham},
  volume    = {10401},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer, Cham},
  abstract  = {SHA-1 is a widely used 1995 NIST cryptographic hash function standard that was officially deprecated by NIST in 2011 due to fundamental security weaknesses demonstrated in various analyses and theoretical attacks. Despite its deprecation, SHA-1 remains widely used in 2017 for document and TLS certificate signatures, and also in many software such as the GIT versioning system for integrity and backup purposes. 
A key reason behind the reluctance of many industry players to replace SHA-1 with a safer alternative is the fact that finding an actual collision has seemed to be impractical for the past eleven years due to the high complexity and computational cost of the attack. 
In this paper, we demonstrate that SHA-1 collision attacks have finally become practical by providing the first known instance of a collision. Furthermore, the prefix of the colliding messages was carefully chosen so that they allow an attacker to forge two PDF documents with the same SHA-1 hash yet that display arbitrarily-chosen distinct visual contents. 
We were able to find this collision by combining many special cryptanalytic techniques in complex ways and improving upon previous work. In total the computational effort spent is equivalent to $2^{63.1}$ SHA-1 compressions and took approximately 6 500 CPU years and 100 GPU years. As a result while the computational power spent on this collision is larger than other public cryptanalytic computations, it is still more than 100 000 times faster than a brute force search.},
  doi       = {10.1007/978-3-319-63688-7_19},
  keywords  = {hash function, cryptanalysis, collision attack, collision example, differential path},
  url       = {https://shattered.it},
}

@Misc{stosurvey2017,
  author       = {Kevin Troy},
  title        = {stackoverflow Developer Surver 2017},
  howpublished = {https://stackoverflow.blog/2017/03/22/now-live-stack-overflow-developer-survey-2017-results/},
  month        = mar,
  year         = {2017},
  note         = {Accessed: 2018-01-01},
  url          = {https://insights.stackoverflow.com/survey/2017},
}

@InProceedings{Timm2011,
  author       = {Fabian Timm and Erhardt Barth},
  title        = {Accurate eye centre localisation by means of gradients},
  booktitle    = {Proceedings of the International Conference on Computer Vision Theory and Applications},
  year         = {2011},
  editor       = {Leonid Mestetskiy and Jos\'e Braz},
  volume       = {1},
  series       = {Proceedings of the International Conference on Computer Vision Theory and Applications},
  pages        = {125--130},
  address      = {Vilamoura, Algarve, Portugal},
  month        = mar,
  organization = {INSTICC},
  publisher    = {SciTePress},
  abstract     = {The estimation of the eye centres is used in several computer vision applications such as face recognition or eye tracking. Especially for the latter, systems that are remote and rely on available light have become very popular and several methods for accurate eye centre localisation have been proposed. Nevertheless, these methods often fail to accurately estimate the eye centres in difficult scenarios, e.g. low resolution, low contrast, or occlusions. We therefore propose an approach for accurate and robust eye centre localisation by using image gradients. We derive a simple objective function, which only consists of dot products. The maximum of this function corresponds to the location where most gradient vectors intersect and thus to the eye's centre. Although simple, our method is invariant to changes in scale, pose, contrast and variations in illumination. We extensively evaluate our method on the very challenging BioID database for eye centre and iris localisation. Moreover, we compare our method with a wide range of state of the art methods and demonstrate that our method yields a significant improvement regarding both accuracy and robustness.},
  doi          = {10.5220/0003326101250130},
  keywords     = {Eye centre localisation, pupil and iris localisation, image gradients, feature extraction, shape analysis},
  url          = {http://www.inb.uni-luebeck.de/fileadmin/files/PUBPDFS/TiBa11b.pdf},
}

@InProceedings{Vatahska2007,
  author    = {Teodora Vatahska and Maren Bennewitz and Sven Behnke},
  title     = {{Feature-based Head Pose Estimation from Images}},
  booktitle = {7th IEEE-RAS International Conference on Humanoid Robots},
  year      = {2007},
  address   = {Pittsburgh, PA, USA},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Estimating the head pose is an important capability of a robot when interacting with humans since the head pose usually indicates the focus of attention. In this paper, we present a novel approach to estimate the head pose from monocular images. Our approach proceeds in three stages. First, a face detector roughly classifies the pose as frontal, left, or right profile. Then, classifiers trained with AdaBoost using Haar-like features, detect distinctive facial features such as the nose tip and the eyes. Based on the positions of these features, a neural network finally estimates the three continuous rotation angles we use to model the head pose. Since we have a compact representation of the face using only few distinctive features, our approach is computationally highly efficient. As we show in experiments with standard databases as well as with real-time image data, our system locates the distinctive features with a high accuracy and provides robust estimates of the head pose.},
  doi       = {10.1109/ICHR.2007.4813889},
  keywords  = {Head, Face detection, Human robot interaction, Focusing, Detectors, Computer vision, Facial features, Nose, Eyes, Neural Networks},
  url       = {http://hrl.informatik.uni-freiburg.de/papers/vatahska07humanoids.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;bibtexkey;false;author;false;year;false;}
