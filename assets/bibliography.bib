% Encoding: UTF-8

@Article{Dalmaijer2014,
  author   = {Edwin Dalmaijer and Sebastiaan Math\^ot and Stefan van der Stigchel},
  title    = {{PyGaze}: an open-source, cross-platform toolbox for minimal-effort programming of eye tracking experiments},
  journal  = {Behavior Research Methods},
  year     = {2014},
  volume   = {46},
  number   = {4},
  pages    = {913--921},
  month    = dec,
  abstract = {The PyGaze toolbox is an open-source software package for Python, a high-level programming language. It is designed for creating eyetracking experiments in Python syntax with the least possible effort, and it offers programming ease and script readability without constraining functionality and flexibility. PyGaze can be used for visual and auditory stimulus presentation; for response collection via keyboard, mouse, joystick, and other external hardware; and for the online detection of eye movements using a custom algorithm. A wide range of eyetrackers of different brands (EyeLink, SMI, and Tobii systems) are supported. The novelty of PyGaze lies in providing an easy-to-use layer on top of the many different software libraries that are required for implementing eyetracking experiments. Essentially, PyGaze is a software bridge for eyetracking research.},
  doi      = {10.3758/s13428-013-0422-2},
  keywords = {Eyetracking, Open-source, Software, Python, PsychoPy, Gaze contingency, read},
  url      = {https://link.springer.com/article/10.3758%2Fs13428-013-0422-2},
}

@InCollection{Davson2017,
  author    = {Hugh Davson},
  title     = {Human Eye},
  booktitle = {Encyclopædia Britannica},
  publisher = {Encyclopædia Britannica, inc.},
  year      = {2017},
  month     = sep,
  note      = {Accessed: December 11, 2017},
  abstract  = {Human eye, in humans, specialized sense organ capable of receiving visual images, which are then carried to the brain.},
  url       = {https://www.britannica.com/science/human-eye},
}

@Misc{Facebase,
  author       = {Seth M. Weinberg and Mary L. Marazita},
  title        = {3D Facial Norms Database},
  howpublished = {https://www.facebase.org},
  month        = nov,
  year         = {2009},
  note         = {NIDCR Grant: 1-U01-DE020078},
  abstract     = {Although ample evidence exists that facial appearance and structure are highly heritable, there is a dearth of information regarding how variation in specific genes relates to the diversity of facial forms evident in our species. With the advent of affordable, non-invasive 3D surface imaging technology, it is now possible to capture detailed quantitative information about the face in a large number of individuals. By coupling state- of-the-art 3D imaging with advances in high-throughput genotyping, an unparalleled opportunity exists to map the genetic determinants of normal facial variation. An improved understanding of the relationship between genotype and facial phenotype may help illuminate the factors influencing liability to common craniofacial anomalies, particularly orofacial clefts, which are among the most prevalent birth defects in humans.},
  url          = {https://www.facebase.org/facial_norms},
}

@InProceedings{Filho2010,
  author    = {H\'elio Perroni Filho and Alberto Ferreira De Souza},
  title     = {{VG-RAM WNN} approach to monocular depth perception},
  booktitle = {Neural Information Processing. Models and Applications},
  year      = {2010},
  editor    = {Kok Wai Wong and Balapuwaduge Sumudu Udaya Mendis and Abdesselam Bouzerdoum},
  volume    = {6444},
  series    = {Lecture Notes in Computer Science},
  pages     = {509--516},
  address   = {Berlin/Heidelberg, Germany},
  month     = nov,
  publisher = {Springer},
  abstract  = {We have examined Virtual Generalizing Random Access Memory Weightless Neural Networks (VG-RAM WNN) as platform for depth map inference from static monocular images. For that, we have designed, implemented and compared the performance of VG-RAM WNN systems against that of depth estimation systems based on Markov Random Field (MRF) models. While not surpassing the performance of such systems, our results are consistent to theirs, and allow us to infer important features of the human visual cortex.},
  doi       = {10.1007/978-3-642-17534-3_63},
  keywords  = {Monocular depth perception, weightless neural networks},
  url       = {https://www.researchgate.net/publication/220000031_VG-RAM_WNN_approach_to_monocular_depth_perception},
}

@InProceedings{Judd2009,
  author    = {Tilke Judd and Krista Ehinger and Fr\'edo Durand and Antonio Torralba},
  title     = {{Learning to Predict Where Humans Look}},
  booktitle = {IEEE 12th International Conference on Computer Vision (ICCV)},
  year      = {2009},
  pages     = {2106--2113},
  month     = sep,
  abstract  = {For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.},
  doi       = {10.1109/ICCV.2009.5459462},
  url       = {http://people.csail.mit.edu/tjudd/WherePeopleLook/},
}

@Article{Jun2016,
  author   = {Dongwook Jun and Jong Man Lee and Su Yeong Gwon and Weiyuan Pan and Hyeon Chang Lee and Kang Ryoung Park and Hyun-Cheol Kim},
  title    = {{Compensation Method of Natural Head Movement for Gaze Tracking System Using an Ultrasonic Sensor for Distance Measurement}},
  journal  = {Sensors},
  year     = {2016},
  volume   = {16},
  number   = {110},
  month    = jan,
  abstract = {Most gaze tracking systems are based on the pupil center corneal reflection (PCCR) method using near infrared (NIR) illuminators. One advantage of the PCCR method is the high accuracy it achieves in gaze tracking because it compensates for the pupil center position based on the relative position of corneal specular reflection (SR). However, the PCCR method only works for user head movements within a limited range, and its performance is degraded by the natural movement of the user's head. To overcome this problem, we propose a gaze tracking method using an ultrasonic sensor that is robust to the natural head movement of users. Experimental results demonstrate that with our compensation method the gaze tracking system is more robust to natural head movements compared to other systems without our method and commercial systems.},
  doi      = {10.3390/s16010110},
  keywords = {gaze tracking system, compensation of head movements, ultrasonic sensor, natural head movement},
  url      = {http://www.mdpi.com/1424-8220/16/1/110},
}

@Article{King2009,
  author   = {Davis E. King},
  title    = {{Dlib-ml: A Machine Learning Toolkit}},
  journal  = {Journal of Machine Learning Research},
  year     = {2009},
  volume   = {10},
  pages    = {1755--1758},
  month    = jul,
  note     = {\url{http://dlib.net/}},
  abstract = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classifi- cation, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
  keywords = {kernel-methods, svm, rvm, kernel clustering, C++, Bayesian networks},
  url      = {http://jmlr.csail.mit.edu/papers/volume10/king09a/king09a.pdf},
}

@InProceedings{PaulaVeronese2012,
  author    = {Lucas de Paula Veronese and Lauro Jos\'e Lyrio Junior and Filipe Wall Mutz and Jorcy de Oliveira Neto and Vitor Barbirato Azevedo and Mariella Berger and Alberto Ferreira De Souza and Claudine Badue},
  title     = {{Stereo matching with VG-RAM Weightless Neural Networks}},
  booktitle = {12th International Conference on Intelligent Systems Design and Applications (ISDA)},
  year      = {2012},
  editor    = {Ajith Abraham and Albert Zomaya and Sebastian Ventura and Ronald Yager and Vaclav Snasel and Azah Kamilah Muda and Philip Samuel},
  pages     = {309--314},
  address   = {Kochi, India},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Virtual Generalizing Random Access Memory Weightless Neural Networks (VG-RAM WNN) is an effective machine learning technique that offers simple implementation and fast training and test. We examined the performance of VG-RAM WNN on binocular dense stereo matching using the Middlebury Stereo Datasets. Our experimental results showed that, even without tackling occlusions and discontinuities in the stereo image pairs examined, our VG-RAM WNN architecture for stereo matching was able to rank at 114th position in the Middlebury Stereo Evaluation system. This result is promising, because the difference in performance among approaches ranked in distinct positions is very small.},
  doi       = {10.1109/ISDA.2012.6416556},
  keywords  = {Binocular Dense Stereo Matching, VG-RAM Weightless Neural Networks, Middlebury Stereo Vision Page},
  url       = {https://www.researchgate.net/publication/261468052_Stereo_matching_with_VG-RAM_Weightless_Neural_Networks},
}

@InProceedings{Roussos2012,
  author       = {Anastasios Roussos and Chris Russell and Ravi Garg and Lourdes Agapito},
  title        = {{Dense Multibody Motion Estimation and Reconstruction from a Handheld Camera}},
  booktitle    = {2013 IEEE International Symposium on Mixed and Augmented Reality},
  year         = {2012},
  pages        = {31--40},
  address      = {Atlanta, GA, USA},
  month        = nov,
  organization = {IEEE},
  publisher    = {Piscataway},
  abstract     = {Existing approaches to camera tracking and reconstruction from a single handheld camera for Augmented Reality (AR) focus on the reconstruction of static scenes. However, most real world scenarios are dynamic and contain multiple independently moving rigid objects. This paper addresses the problem of simultaneous segmentation, motion estimation and dense 3D reconstruction of dynamic scenes. We propose a dense solution to all three elements of this problem: depth estimation, motion label assignment and rigid transformation estimation directly from the raw video by optimizing a single cost function using a hill-climbing approach. We do not require prior knowledge of the number of objects present in the scene -- the number of independent motion models and their parameters are automatically estimated. The resulting inference method combines the best techniques in discrete and continuous optimization: a state of the art variational approach is used to estimate the dense depth maps while the motion segmentation is achieved using discrete graph-cut based optimization. For the rigid motion estimation of the independently moving objects we propose a novel tracking approach designed to cope with the small fields of view they induce and agile motion. Our experimental results on real sequences show how accurate segmentations and dense depth maps can be obtained in a completely automated way and used in marker-free AR applications.},
  doi          = {10.1109/ISMAR.2012.6402535},
  keywords     = {Cameras, Optimization, Motion segmentation, Image reconstruction, Tracking, Estimation, Motion estimation},
  url          = {https://www.researchgate.net/publication/233981640_Dense_Multibody_Motion_Estimation_and_Reconstruction_from_a_Handheld_Camera},
}

@InProceedings{Timm2011,
  author       = {Fabian Timm and Erhardt Barth},
  title        = {Accurate eye centre localisation by means of gradients},
  booktitle    = {Proceedings of the International Conference on Computer Vision Theory and Applications},
  year         = {2011},
  editor       = {Leonid Mestetskiy and Jos\'e Braz},
  volume       = {1},
  series       = {Proceedings of the International Conference on Computer Vision Theory and Applications},
  pages        = {125--130},
  address      = {Vilamoura, Algarve, Portugal},
  month        = mar,
  organization = {INSTICC},
  publisher    = {SciTePress},
  abstract     = {The estimation of the eye centres is used in several computer vision applications such as face recognition or eye tracking. Especially for the latter, systems that are remote and rely on available light have become very popular and several methods for accurate eye centre localisation have been proposed. Nevertheless, these methods often fail to accurately estimate the eye centres in difficult scenarios, e.g. low resolution, low contrast, or occlusions. We therefore propose an approach for accurate and robust eye centre localisation by using image gradients. We derive a simple objective function, which only consists of dot products. The maximum of this function corresponds to the location where most gradient vectors intersect and thus to the eye's centre. Although simple, our method is invariant to changes in scale, pose, contrast and variations in illumination. We extensively evaluate our method on the very challenging BioID database for eye centre and iris localisation. Moreover, we compare our method with a wide range of state of the art methods and demonstrate that our method yields a significant improvement regarding both accuracy and robustness.},
  doi          = {10.5220/0003326101250130},
  keywords     = {Eye centre localisation, pupil and iris localisation, image gradients, feature extraction, shape analysis},
  url          = {http://www.inb.uni-luebeck.de/fileadmin/files/PUBPDFS/TiBa11b.pdf},
}

@InProceedings{Vatahska2007,
  author    = {Teodora Vatahska and Maren Bennewitz and Sven Behnke},
  title     = {{Feature-based Head Pose Estimation from Images}},
  booktitle = {7th IEEE-RAS International Conference on Humanoid Robots},
  year      = {2007},
  address   = {Pittsburgh, PA, USA},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Estimating the head pose is an important capability of a robot when interacting with humans since the head pose usually indicates the focus of attention. In this paper, we present a novel approach to estimate the head pose from monocular images. Our approach proceeds in three stages. First, a face detector roughly classifies the pose as frontal, left, or right profile. Then, classifiers trained with AdaBoost using Haar-like features, detect distinctive facial features such as the nose tip and the eyes. Based on the positions of these features, a neural network finally estimates the three continuous rotation angles we use to model the head pose. Since we have a compact representation of the face using only few distinctive features, our approach is computationally highly efficient. As we show in experiments with standard databases as well as with real-time image data, our system locates the distinctive features with a high accuracy and provides robust estimates of the head pose.},
  doi       = {10.1109/ICHR.2007.4813889},
  keywords  = {Head, Face detection, Human robot interaction, Focusing, Detectors, Computer vision, Facial features, Nose, Eyes, Neural Networks},
  url       = {http://hrl.informatik.uni-freiburg.de/papers/vatahska07humanoids.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;bibtexkey;false;author;false;year;false;}
